{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affective Computing - Programming Assignment 2\n",
    "\n",
    "### Objective\n",
    "\n",
    "Your task is to extract a set of prosodic correlates (i.e. suprasegmental speech parameters) and cepstral features from speech recordings. Then, an emotion recognition system is constructed to recognize happy versus sad emotional speech (a quite easy two class problem) using a simple supervised classifier training and testing structure.\n",
    "\n",
    "The original speech data is a set of simulated emotional speech (i.e. acted) from ten speakers speaking five different pre-segmented sentences of roughly 2-3 seconds in two different emotional states (happy and sad) totaling 100 samples.\n",
    "Basic prosodic features (i.e. distribution parameters derived from the prosodic correlates) are extracted using a simple voiced/unvoiced analysis of speech, pitch tracker, and energy analysis. Another set of Mel-Frequency Cepstral Coefficients (MFCC) features are also calculated for comparison. \n",
    "\n",
    "To produce speaker independent and content dependent emotion recognition case (i.e. while a same persons samples are not included in both training and testing sets, the same sentences are spoken in both the training and the testing sets) that could correspond to a public user interface with specific commands.\n",
    "\n",
    "Support Vector Machine (SVM) classifiers are trained. A random subset of 1/2 of the available speech data (i.e. half of the persons) is used to train the emotion recognition system, first using a set of simple prosodic parameter features and then a classical set of MFCC derived features. The rest of the data (the other half of the persons) is then used to evaluate the performances of the trained recognition systems.\n",
    "\n",
    "<!--### Implementation\n",
    "<!---The data and toolbox files used in this exercise are:\n",
    "Study the toolbox functions (e.g. ‘getF0’, ‘melcepst’) and the generic MATLAB functions (e.g. ‘hamming’, ‘conv’, ‘resample’, ‘filter’, ‘mean’, ‘std’, ‘prctile’, ‘kurtosis’, ‘sum’, ‘length’, ‘linspace’, ‘trainsvm’, ‘svmclassify’, and ‘confusionmat’) as they are needed in the exercise.-->\n",
    "\n",
    "<!--Nine dictionaries are stored in the provided data file:-->\n",
    "\n",
    "<!--* speech_sample\n",
    "* testing_class \n",
    "* testing_data_mfcc \n",
    "* testing_data_proso \n",
    "* testing_personID \n",
    "* training_class \n",
    "* training_data_mfcc \n",
    "* training_data_proso \n",
    "* training_personID -->\n",
    "\n",
    "<!--To access one dictionary, using [`scipy.io`](https://docs.scipy.org/doc/scipy/reference/io.html) library for example: scipy.io.loadmat('filePath')['dictoionaryName']. **speech_sample** is used in the feature extraction part and the pre-extracted features in the emotion recognition part of this lab are **testing_class**, **testing_data_mfcc**, **testing_personID**, **training_class**, **training_data_mfcc**, **training_data_proso**, **training_personID**.-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0. Preparation\n",
    "Downsample the ‘speech_sample’ from the original Fs of 48 kHz to 11.025 kHz using [`scipy.signal.resample()`](https://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.signal.resample.html) function.\n",
    "\n",
    "**Steps**:\n",
    "1. Load data 'speech_sample' from file *lab2_data.mat*. It is better to make sure the sample is a 1-D time series.\n",
    "2. Declare the sampling frequency of the original signal, and your new sampling frequency.\n",
    "3. Resample the signal using [`scipy.signal.resample()`](https://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.signal.resample.html)\n",
    "4. Visualize the resampled signal. Please make a appropriate time vector as the x axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--### Task 0.1. Load Data\n",
    "Load the ‘speech_sample’ from the provided dataset containing a raw speech waveform and do the following (Note, the sampling rate (fs) of the sample speech signal is 48 kHz):-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Load the 'speech_sample'\n",
    "exerciseData = sio.loadmat('lab2_data.mat')\n",
    "speechSample = exerciseData['speech_sample'].reshape(-1)\n",
    "\n",
    "\n",
    "# 2. Declare the source sampling frequency, and the target sampling frequency.\n",
    "#    2.1 Source sampling frequency\n",
    "fs_source =\n",
    "\n",
    "#    2.2 Target sampling frequency\n",
    "# Target frequency\n",
    "fs_down = \n",
    "\n",
    "# 3. Downsample the speech sample\n",
    "speech_resampled = signal.resample(speechSample, np.round(len(speechSample) * fs_down / fs_source).astype('int'))\n",
    "\n",
    "\n",
    "# 4. Visualize the downsampled speech signal.\n",
    "#    4.1 Creating the corresponding time vector, whose length is same to the length to the given signal. \n",
    "#        You can use np.linspace() function to perform this. For example\n",
    "\n",
    "#    4.2 Plot your result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1 MFCC calculations using the provided sample speech signal.\n",
    "\n",
    "**Steps**:\n",
    "1. Pre-emphasize the resampled signal by applying a high pass filter, using the [`scipy.signal.lfilter()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.lfilter.html) function.\n",
    "   Apply a pre-emphasis filter $$ H(z) = 1-a \\times z^{-1} $$ with α=0.98 to emphasize higher frequencies in your downsampled speech signal (Tip: use [`scipy.signal.lfilter`](https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.signal.lfilter.html)). \n",
    "   \n",
    "   Hint for defining the filter: you will provide two vectors **b** and **a** to define the filter, **a** for the denominator and **b** for the numerator. So finally your filter will be defined as $$H(z) = \\frac{b[0] z^0 + b[1] z^{-1} + ... + b[i] z^{-i}+...}{a[0] z^0 + a[1] z^{-1} + ... + a[i] z^{-i}+...}$$\n",
    "2. Extract the 12 mfcc coefficient by using the [`python_speech_features.mfcc()`](http://python-speech-features.readthedocs.io/en/latest/) function.\n",
    "    1. **The [`python_speech_features.mfcc()`](http://python-speech-features.readthedocs.io/en/latest/) function has its internal pre-emphasis functionality. However, we do the pre-emphasis beforehand in order to have a better understanding on it.**\n",
    "3. Visualize the 12 mfcc coefficient contours. Please also make the corresponding time vector as the x axis.\n",
    "4. Calculate the mean of each contour using [`numpy.mean(axis=)`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import lfilter\n",
    "from python_speech_features import mfcc\n",
    "# 1. Pre-emphasize your resampled signal.\n",
    "#    1.1 Define the polynomial of your fitler\n",
    "#        filter coefficients b, which is the numerator\n",
    "#        filter coefficients a, which is the denominator\n",
    "a = \n",
    "b = \n",
    "\n",
    "#    1.2 Apply the filter on the signal\n",
    "preEmphasizedSample = lfilter(..., ..., ...)\n",
    "\n",
    "# 2. Extract the mfcc coefficient by callying the mfcc() function\n",
    "     # remeber to set the pre-emphasize argument to 0 since the signal has been pre-emphasized.\n",
    "frameLen = int(2 ** np.floor(np.log2(0.03*fs_down)))\n",
    "mfccContour = mfcc(preEmphasizedSample, \\\n",
    "                   fs_down, \\\n",
    "                   winlen = float(frameLen)/fs_down, \\\n",
    "                   winstep = float(frameLen)/(2*fs_down), \\\n",
    "                   numcep = 12, \\\n",
    "                   preemph = 0)\n",
    "\n",
    "# 3. Plot the 12 mfcc contours\n",
    "#    3.1 Create the time vector for the MFCC contour. \n",
    "#        Again, using the np.linspace() function.\n",
    "#        However, please note the length of the resulted mfcc contour is different to the sample length.\n",
    "#        Thus, the length of your time vector here should be the length of the mfcc contour\n",
    "#        The ending point can be computed by the: a) number of mfcc samples, and b) time length of samples used\n",
    "#        for calculating one mfcc sample.\n",
    "#        \n",
    "#        For example, here we set the frame increment length is 128 samples, corresponding roughly to 11.6ms frames \n",
    "#        at Fs=11025Hz. So for inputs of the np.linspace() function, the ending point is mfccNum * 11.6ms around.\n",
    "\n",
    "\n",
    "#    3.2 Visualize your MFCC contour\n",
    "\n",
    "\n",
    "# 4. Calculate the mean for each contour.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1. Why do we need to pre-emphasize the speech signal before computing the MFCC feature?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 Extract the Intensity/Energy parameter\n",
    "Firstly, please calculate the short time energy (STE) of the downsampled ‘speech_sample’ using the squared signal $x(t)^2$ and a 0.01s hamming window frames (Note! the extra length of the window. Clip half a window length from the beginning and at the end). Then calculate 5 distribution parameter features from the utterance (the signal).\n",
    "\n",
    "\n",
    "**Steps**:\n",
    "1. Define a hamming window using the [`scipy.signal.hamming()`](https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.signal.hamming.html) function. The window length is the number of frames in 0.01s.\n",
    "\n",
    "2. Apply the hamming window to convolve the squared signal, using the [`scipy.signal.convolve()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve.html) function. The convolution result is the short time energy (STE) controu.\n",
    "3. Clip half window of frames from the begining and ending of STE contour.\n",
    "4. Visualize the resulted STE controur. Please also include the time axis\n",
    "5. Calculating the following 5 distribution parameter feature from the STE contour:\n",
    "    1. Mean, using the [`numpy.mean(https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.mean.html)`]() function.\n",
    "    2. Standard Deviation (SD), using the [`numpy.std()`](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.std.html) function.\n",
    "    3. 10% percentile, using the [`numpy.percentile()`](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.percentile.html) function.\n",
    "    4. 90% percentile, using the [`numpy.percentile()`](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.percentile.html) function.\n",
    "    5. Kurtosis, using the [`scipy.stats.kurtosis()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kurtosis.html) function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kurtosis\n",
    "# 1. Define a hamming window\n",
    "#    1.1 Calculate the window length, which the number of frames within 0.01s\n",
    "hammingLength = \n",
    "\n",
    "#    1.2 Define the hamming window using signal.hamming()\n",
    "hammingWindow = signal.hamming(hammingLength)\n",
    "\n",
    "\n",
    "# 2. Calculate the short time energy (STE) contour by convolve the hamming window and the squared signal, \n",
    "#    using the scipy.signal.convolve() function\n",
    "ste = signal.convolve(..., \\\n",
    "                      hammingWindow)\n",
    "\n",
    "\n",
    "# 3. Clip half window of frames from both the beginning and end of the STE contour\n",
    "\n",
    "\n",
    "\n",
    "# 4. Visualize the final STE contour.\n",
    "#    4.1 Create the time vector for x-axis\n",
    "\n",
    "#    4.2 Visualize the STE contour\n",
    "\n",
    "\n",
    "\n",
    "# 5. Calculate the 5 distribution parameter feature the of STE contour\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2. Why do we need to clip out half a frame from both beginning and ending of the STE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3. Extract the Pitch/F0 feature\n",
    "**Steps**:\n",
    "1. Extract the Pitch/F0 contour of the resampled speech signal using the **getF0()** function in 0.01s frames. The function is provided in the *f0Lib.py* file.\n",
    "2. Visualize the F0 contour (including the time axis).\n",
    "3. Extract the 5 distribution parameter features of the extracted F0 countour.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from scipy import signal\n",
    "import scipy\n",
    "from f0Lib import getF0\n",
    "\n",
    "# 1. Extract the F0 contour\n",
    "f0,_,T_ind,_ = getF0(..., ...)\n",
    "\n",
    "# 2. Visualize the F0 contour\n",
    "#    2.1 The time vector can be acquired from the the third returned value of the getF0() function\n",
    "#        For example T_ind. The time vector can be computed by dividing the the first column of T_ind \n",
    "#        with the sampling frequency\n",
    "\n",
    "\n",
    "#    2.2 Visulize the F0 contour.\n",
    "\n",
    "\n",
    "\n",
    "# 3. Calculate these distribution parameter features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.4. Extract the Rhythm/Durations parameter\n",
    "**Steps**:\n",
    "1. Perform a Voiced/Unvoiced speech segmentation of speech signal. Tip: Unvoiced frames are marked with 0 F0 values, you can find the voiced frames (i.e. F0 > 0) using [`numpy.where()`](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.where.html).\n",
    "2. From the segmentation, calculate the means and SDs of both Voiced and Unvoiced segment lengths (i.e. voiced segment mean length, SD of voiced segment lengths, unvoiced segment mean length, SD of unvoiced segment lengths).\n",
    "3. Calculate also the voicing ratio, i.e. the ratio of voiced segments versus total segments (Tip: You can do this simply by using the frames).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Segmenting the voiced and unvoiced speech segements.\n",
    "#    1.1 Example on extracting voiced segment lengths\n",
    "framesInd_voiced = np.where(f0>0)[0]\n",
    "diff = framesInd_voiced[1:] - framesInd_voiced[0:-1]\n",
    "voiceToUnviceInd = np.where(diff > 1)[0]\n",
    "voice_seg_num = len(voiceToUnviceInd) + 1\n",
    "voice_seg_lengths = np.zeros(voice_seg_num)\n",
    "tmp = framesInd_voiced[0]\n",
    "\n",
    "for i in range(voice_seg_num - 1):\n",
    "    voice_seg_lengths[i] = framesInd_voiced[voiceToUnviceInd[i]] - tmp + 1\n",
    "    tmp = framesInd_voiced[voiceToUnviceInd[i] + 1]\n",
    "    \n",
    "voice_seg_lengths[-1] = framesInd_voiced[-1] - tmp + 1\n",
    "\n",
    "#####################################################################\n",
    "###################################################################\n",
    "#    1.2 Extract unvoiced segment lengths.\n",
    "\n",
    "\n",
    "\n",
    "# 2. Calculate the means and SDs of both Voiced and Unvoiced segment lengths\n",
    "\n",
    "\n",
    "\n",
    "# 3. Calculate the voicing ratio.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.5. Check your extracted feature\n",
    "1. Print your calculated 12 MFCC coefficients.\n",
    "2. Print the distribution parameter feature of the STE contour.\n",
    "3. Print the distribution parameter feature of the F0 contour.\n",
    "4. Print the 5 prodosic features: **mean** and **std** of lengths of **voiced speeches** and **unvoiced speech**, as well as the **voicing ratio**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Print the 12 MFCC coefficients\n",
    "\n",
    "# 2. Print the distribution paremeter feature of the STE contour\n",
    "\n",
    "# 3. Print the distribution parameter feature of the F0 contour\n",
    "\n",
    "# 3. Print the 5 prodosic features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task 2. Speech Emotion Classification\n",
    "\n",
    "In this part, you we will the [`sklearn.svm`](http://scikit-learn.org/stable/modules/svm.html) library to perform the speech signal classification. The **‘training_data_proso’** and **‘training_data_mfcc’** matrixes contain the calculated prosodic features for the training set (9 features in each row representing a speech sample) and MFCC derived features (12 features) respectively. The **‘training_class’** group vector contains the class of samples: 1 = happy, 2 = sad; corresponding to the rows of the training data matrices.\n",
    "\n",
    "In this part, you will get familiar to three kinds of classifiers, namely the SVM classifier, the Random Forest classifier, and the Neural Network classifer (a multi-layer perceptron).\n",
    "\n",
    "<!---\n",
    "Test the classifiers and plot confusion matrices.\n",
    "* Use the ‘svmclassify’ function (and your trained SVM structures) to classify the ‘training_data_*’ and the ‘testing_data_*’ data matrices. Then, calculate average classification performances for both training and testing data. The correct class labels corresponding with the rows of the training and testing data matrices are in the variables ‘training_class’ and ‘testing_class’, respectively.\n",
    "    * \tCalculate the average classification performances for the training data (‘training_data_proso’ and ‘training_data_mfcc’) using the corresponding prosody and MFCC trained SVMs.\n",
    "    * \tCalculate the average classification performance for the testing data (‘testing_data_proso’ and testing_data_mfcc’) using the corresponding prosody and MFCC trained SVMs.\n",
    "* Plot confusion matrices for the training and testing data for both classifiers. Tip, use ‘confusionmat’ function.-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---speech_sample\n",
    "testing_class\n",
    "testing_data_mfcc\n",
    "testing_data_proso\n",
    "testing_personID\n",
    "training_class\n",
    "training_data_mfcc\n",
    "training_data_proso\n",
    "training_personID\n",
    "### Task 2.1. Preparing your data\n",
    "Dictionaries of the data are listed below:\n",
    "* speech_sample\n",
    "* testing_class\n",
    "* testing_data_mfcc\n",
    "* testing_data_proso\n",
    "* testing_personID\n",
    "* training_class\n",
    "* training_data_mfcc\n",
    "* training_data_proso\n",
    "* training_personID\n",
    "Use [`scipy.io.loadmat()`] to read the dataset.-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1. Train and evaluate your SVM classifiers\n",
    "**Steps**:\n",
    "1. Load training data.\n",
    "2. Train a SVM with the prosody data using the **‘training_data_proso’** features and a **3rd order polynomial** kernel.\n",
    "3. Train a SVM with the MFCC data using the **‘training_data_mfcc’** features and a **3rd order polynomial** kernel.\n",
    "4. Load testing data\n",
    "5. Calculate the average classification accuracy for the training data (**‘training_data_proso’** and **‘training_data_mfcc’**) using the corresponding prosody and MFCC trained SVMs.\n",
    "6. Calculate the average classification accuracy for the testing data (**‘testing_data_proso’** and **‘testing_data_mfcc’**) using the corresponding prosody and MFCC trained SVMs.\n",
    "7. Print the four accuracies you have calculated.\n",
    "8. Plot confusion matrices for the training and testing data for both classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialzing your SVM classifiers.\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# 1. Load data\n",
    "exerciseData = sio.loadmat('lab2_data.mat')\n",
    "\n",
    "#    1.1 Load 'training_data_proso'\n",
    "training_data_proso = exerciseData['training_data_proso']\n",
    "\n",
    "#    1.2 Load 'training_data_mfcc'\n",
    "training_data_mfcc = exerciseData['training_data_mfcc']\n",
    "\n",
    "\n",
    "#    1.3 Load 'training_class'\n",
    "training_class = exerciseData['training_class'].reshape(-1)\n",
    "\n",
    "\n",
    "# 2. Train your classifier using the prodosic data\n",
    "#    2.1 Initialize your svm classifer\n",
    "\n",
    "\n",
    "#    2.2 Train you classifier\n",
    "\n",
    "\n",
    "# 3. Train your classifer using the mfcc data\n",
    "#    3.1 Initialize your svm classifer\n",
    "\n",
    "\n",
    "#    3.2 Train you classifier\n",
    "\n",
    "\n",
    "# 4. Load testing data\n",
    "testing_data_mfcc = exerciseData['testing_data_mfcc']\n",
    "testing_data_proso = exerciseData['testing_data_proso']\n",
    "testing_class = exerciseData['testing_class'].reshape(-1)\n",
    "\n",
    "# 5. Calculate the average classification performances for the training data\n",
    "\n",
    "\n",
    "\n",
    "# 6. Calculate the average classification performance for the testing data\n",
    "\n",
    "\n",
    "\n",
    "# 7. Print the four accuracies.\n",
    "\n",
    "\n",
    "# 8. Visulize the confusion matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--### Task 2.2. Test your classifiers\n",
    "Classify the **‘training_data_*’** and the **‘testing_data_*’** data matrices. Then, calculate average classification performances for both training and testing data. The correct class labels corresponding with the rows of the training and testing data matrices are in the variables ‘training_class’ and ‘testing_class’, respectively.\n",
    "**Steps**:-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--### Task 2.3. Plot confusion matrices for the training and testing data for both classifiers. \n",
    "Print following confusion matrix(Tip, use [`sklearn.metrics.confusion_matrix`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) function):\n",
    "* The confusion matrix of the prosody trained SVM using the **‘training_data_proso’**.\n",
    "* The confusion matrix of the prosody trained SVM using the **‘testing_data_proso’**.\n",
    "* The confusion matrix of the MFCC trained SVM using the **‘training_data_mfcc’**.\n",
    "* The confusion matrix of the MFCC trained SVM using the **‘testing_data_mfcc’**.-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 Train and evaluate a Random Forest Classifier\n",
    "Please train one Random Forest classifer for each of the Prosodic and MFCC feature using the [`sklearn.ensemble.RandomForestClassifier()`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), and print the classification accuracies and confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 1. Train your classifier using the prodosic data\n",
    "#    1.1 Initialize your random forest classifer\n",
    "\n",
    "#    1.2 Train you classifier\n",
    "\n",
    "\n",
    "# 2. Train your classifer using the mfcc data\n",
    "#    2.1 Initialize your random forest classifer\n",
    "\n",
    "#    2.2 Train you classifier\n",
    "\n",
    "\n",
    "# 3. Calculate the average classification performances for the training data\n",
    "\n",
    "\n",
    "\n",
    "# 4. Calculate the average classification performance for the testing data\n",
    "\n",
    "\n",
    "# 5. Print the confusion matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3 Train and evaluate a Neural Network Classifier\n",
    "Please train a multi-layer perceptron for each of the Prosodic and MFCC feature using the [`sklearn.neural_network.MLPClassifier()`](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html), and print the classification accuracies and confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 1. Train your classifier using the prodosic data\n",
    "#    1.1 Initialize your mlp classifer\n",
    "mlpClassifier_proso = MLPClassifier(solver='lbfgs', alpha=1e-4, hidden_layer_sizes=(50, 50, 50), random_state=1, max_iter=20000)\n",
    "\n",
    "#    1.2 Train you classifier\n",
    "\n",
    "\n",
    "\n",
    "# 2. Train your classifer using the mfcc data\n",
    "#    2.1 Initialize your mlp classifer\n",
    "mlpClassifier_mfcc = MLPClassifier(solver='lbfgs', alpha=1e-4,  hidden_layer_sizes=(50, 50, 50), random_state=1, max_iter=20000)\n",
    "\n",
    "#    2.2 Train you classifier\n",
    "\n",
    "\n",
    "\n",
    "# 3. Calculate the average classification performances for the training data\n",
    "\n",
    "\n",
    "\n",
    "# 4. Calculate the average classification performance for the testing data\n",
    "\n",
    "\n",
    "\n",
    "# 5. Print the confusion matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. (OPTIONAL) Speech Emotion Classification using SVM, Subject Independent\n",
    "Generate a person independent 10-fold cross-validation (CV) estimate of the emotion recognition system performance.\n",
    "* Join the training/testing data matrices and the class vectors. Combine also the ‘training_data_personID’ and ‘testing_data_personID’ vectors that are needed to make the CV folds.\n",
    "\n",
    "* Construct the CV folds by training ten SVMs. For each SVM nine persons’ data is used as the training set (i.e. 90 samples) and one persons’ samples are kept as the test set (i.e. 10 samples) for the respective fold (i.e. each SVM has different persons’ samples excluded from the training set). Test each ten trained SVMs by using the corresponding one held-out persons’ samples and then calculate the average classification performances for each fold.\n",
    "\n",
    "* Calculate the mean and SD of the ten CV fold performances to produce the final CV performance estimate of the emotion recognition system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
