{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affective Computing - Programming Assignment 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "\n",
    "The objective of the exercise is to build the facial expression recognition system. The system includes face preprocessing, feature extraction and classification. In the exercise, you will learn how to preprocess the facial expression image, extract the feature from an image or a video, and classify the video into one category.\n",
    "\n",
    "Specifically, the region of interest (i.e., facial image) is extracted using face tracking, face registration and face crop functions.  Basic spatiotemporal features (i.e., LBP-TOP features) are extracted using LBP-TOP. To produce emotion recognition case, Support Vector Machine (SVM) classifiers are trained.  50 videos from 5 participants are used to train the emotion recognition, use spatiotemporal features. The rest of the data (50 videos) is used to evaluate the performances of the trained recognition systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Database\n",
    "The original facial expression data is a sub-set of eNTERFACE (acted facial expression), from ten actors acting **happy** and **sadness** behaviors. The used dataset in the exercise includes 100 facial expression samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---#### Help-->\n",
    "<!--- The data and toolbox files used in this exercise can be found in the Affective Computing course webpage (see the Noppa system).\n",
    "-->\n",
    "\n",
    "<!---In the exercise, you should know that some basic python libries (numpy, scikit-image, scipy, pyploy, sklearn) before the programming. If you have questions, send your question to us: \n",
    "* [Henglin.Shi@oulu.fi](mailto:Henglin.Shi@oulu.fi)\n",
    "* [Yante.Li@oulu.fi](mail.Yante.Li@oulu.fi)-->\n",
    "\n",
    "<!---Use the following website to help the usage of python libries. \n",
    "* numpy: http://www.numpy.org/\n",
    "* skimage: https://scikit-image.org/\n",
    "* scipy: https://www.scipy.org/\n",
    "* matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\n",
    "* sklearn: http://scikit-learn.org/stable/\n",
    "    * sklearn.svm: http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "    * skealrn.metrics.confusion_matrix: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Face preprocessing\n",
    "\n",
    "In this part, you are supposed to process a face image. There are three subtasks you need to do:\n",
    "* **Task 1.1.** Detect face and facial landmarks using the [`DLib`](http://dlib.net/) library.\n",
    "* **Task 1.2.** You are asked to perform a face registration task using a set of fixed landmarks from a standard model, and extract face from the registered image.\n",
    "* **Task 1.3.** Visualize your result using subplots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1. Extract facial landmarks\n",
    "**Steps**:\n",
    "1. Load the example image *exampleImg.jpg*, using the [`skimage.io.imread()`](http://scikit-image.org/docs/dev/api/skimage.io.html#skimage.io.imread) function for example.\n",
    "2. Initialize a face detector and a face landmarks detector. We have provide the code of this part, please learn to use them.\n",
    "3. Detect face\n",
    "4. Detect the face landmarks\n",
    "5. Transfrom the detected result to a 2-D array, we provide the function *shape2points()* below to perform this.\n",
    "\n",
    "*Besides, here is an example for facial landmarks extraction: http://dlib.net/face_landmark_detection.py.html*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading required libraries\n",
    "import dlib\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "\n",
    "\n",
    "# We provide the shape to points function.\n",
    "def shape2points(shape, dtype = 'int', pointNum = 68):\n",
    "    coords = np.zeros((pointNum, 2), dtype=dtype)\n",
    "    for i in range(0, pointNum):\n",
    "        coords[i] = (shape.part(i).x, shape.part(i).y)\n",
    "    return coords\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. Load exampleImg.jpg, using skimage.io.imread()\n",
    "\n",
    "\n",
    "# 2. Initializing face detector and shape predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "\n",
    "# 3. Detecting face, return rectangles, each rectangle corresponds to one face.\n",
    "# You need to fill the missing argument of this function\n",
    "dets = detector(..., 1)\n",
    "\n",
    "# Extracting the shape of the face in the first rectangle (using the first element of the rectangles variable\n",
    "\n",
    "\n",
    "# Extract facial landmarks from shape by calling the shape2points() function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2. Face normalization\n",
    "**Steps**:\n",
    "1. Load the landmark position of a standard face model. We provide these positions in a csv file, and also the code block to read these positions.\n",
    "2. Calculate the transformation between your detected landmarks position and the standard face model landmark positions using the [`skimage.transform.PolynomialTransform()`](http://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.PolynomialTransform) class and its *estimate()* methods. \n",
    "    1. Instantiate a PolynomialTransform object by calling transform.PolynomialTransform()\n",
    "    2. Call its estimate() method to calculate the transformation between the two sets of points. The manual of this method can be found in the same page which introduced of this class. \n",
    "3. Transform the example image using the calculated transformation to register (map) the example image into a space of the standard face model. You can use the [`skimage.transform.warp()`](http://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.warp) function to perform this.\n",
    "4. Crop the face from the registered face using the standard face model landmarks. The cropping function is provided in the *exercise1Lib.py*, you can directly use it after importing. \n",
    "5. Also extract the face from the example image using your detected landmarks.\n",
    "<!---Use **cropFace** to crop the facial image from the background. For details on the **cropFace** function, please read the function in **exercise1Lib.py**. Usually, we have to do the face registration to remove the subject characteristics. To do a  face registration, firstly you need a set of landmarks of a standard face model. Then you calculate the transformation between you detected landmarks and the standard landmarks. Last you apply the transformation on your example image/-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We offer the code to load a standard face model, which contains the mean position of landmarkds estimated from the training data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the landmark position of the standard face model\n",
    "import csv\n",
    "import numpy as np\n",
    "standardModel = np.zeros((68, 2))\n",
    "tmp = 0 \n",
    "with open('mean.csv', 'rb') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in spamreader:\n",
    "        standardModel[tmp, 0] = float(row[0])\n",
    "        standardModel[tmp, 1] = float(row[1])\n",
    "        tmp += 1\n",
    "standardModel = standardModel * 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---**Then you need to calculate the transformation using [`skimage.transform.PolynomialTransform()`](http://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.PolynomialTransform)**. This class has several methods, expecially, you need to use the **estimate()** method to calculate the transformation between two sets of points. **Registering the example face** by transforming the example image into the standard model space, using [`skimage.transform.warp()`](http://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.warp) and you calculated transformation. **Cropping the original example image according to the detected landmarks, and the registered image according to the standard face mode. **-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import transform\n",
    "from exercise1Lib import cropFace\n",
    "\n",
    "# 2. Calculating the transorfmation between the two set of points \n",
    "# 2.1 Instantiating a PolynomialTransform() transform function\n",
    "\n",
    "\n",
    "# 2.2 Calculating the transformation by calling the estimate() method.\n",
    "#     You do not need to retuern any value after calling this methods,\n",
    "#     because the transformation parameter is store in the object you instantiated after calling this methods.\n",
    "\n",
    "\n",
    "# 3. Warping your example image using the transform.warp() function\n",
    "\n",
    "\n",
    "# 4. Crop the face from registered image using the provided cropFace function.\n",
    "\n",
    "\n",
    "# 5. Croping the face from the example image using detected landmarks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3. Display result\n",
    "Here you are asked to draw a figure with 3 x 2 subplots using [`matplotlib.pyplot.subplots()`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplots.html). Please read the manual of it and also the [`matplotlib.pyplot`](https://matplotlib.org/api/pyplot_api.html). Below explains content should be presented in each subplots:\n",
    "* subplot [0,0]: the original example image and detected landmarks.\n",
    "* subplot [1,0]: the face cropped from the example image. \n",
    "* subplot [2,0]: the histogram of the face cropped from the example. \n",
    "\n",
    "*We provide the code for above three subplots, please refer them as examples.* Then you need to implemente:\n",
    "* subplot [0,1]: the registered face image.\n",
    "* subplot [1,1]: the face cropped from the registered face image.\n",
    "* subplot [2,1]: the histogram of the face cropped from the registered face image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "# Constructing figure with 2x3 subplots\n",
    "fig,ax = plt.subplots(3,2, figsize=(15, 15))\n",
    "\n",
    "# subplot [0,0]: show the original example image\n",
    "ax[0,0].imshow(exampleImg)\n",
    "\n",
    "\n",
    "# Placing detected landmarks on subplot [0,0], we provide an exmaple to do this.\n",
    "for pointIte in range(len(keypoints)):\n",
    "    # Create a Rectangle patch\n",
    "    rect = patches.Rectangle((keypoints[pointIte][0] - 1,keypoints[pointIte][1] - 1), \\\n",
    "                                3, 3, linewidth=1,edgecolor='r',facecolor='none')\n",
    "    # Add the patch to the Axes\n",
    "    ax[0, 0].add_patch(rect)\n",
    "\n",
    "    \n",
    "# subplot [1,0]: show the face cropped from the example image.\n",
    "ax[1,0].imshow(cropedExampleFace, 'gray')\n",
    "\n",
    "\n",
    "# subplot [2,0]: show the histogram of the face cropped from the example image.\n",
    "Hist_croppedExampleImg, _  = np.histogram(img_as_ubyte(cropedExampleFace).ravel(), bins=256)\n",
    "ax[2,0].plot(Hist_croppedExampleImg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# subplot [0,1]: show the registered image\n",
    "\n",
    "\n",
    "# place the model landmarks on the registered image\n",
    "\n",
    "\n",
    "# subplot [1,1]: show the face cropped from the registered image\n",
    "\n",
    "\n",
    "# subplot [2,1]: show the histogram of the face cropped from the registered image.\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Feature extraction\n",
    "Here you are asked to extract LBP feature. The algorithm is named as Local Binary Pattern. So far many researcher use it in face recognition, facial expression recognition and texture classification.\n",
    "Here you will use the [`skimage.feature.local_binary_pattern()`](http://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.local_binary_pattern) function to extract the LBP feature. Steps are explained below.\n",
    "\n",
    "**Steps**:\n",
    "1. Define the LBP parameters. Before doing this, please read carefully of its manual.\n",
    "    1.  P: Number of neighbours, plese set P = 8\n",
    "    2.  R: Radius of circle, please set R = 1.0\n",
    "    3.  LBP methods: please set it as 'nri_uniform'\n",
    "\n",
    "2. Extract the LBP face by calling the *skimage.feature.local_binary_pattern()* function\n",
    "\n",
    "3. Calculate the histogram of the LBP face\n",
    "    1. Caculate the histogram of the LBP face\n",
    "    2. Normalize the histogram to make the histogram's sum one one (dividing each element by the sum of the histogram).\n",
    "\n",
    "4. Visualize the result using two subplots.\n",
    "    1. Draw the LBP face at the left one.\n",
    "    2. Draw the normalized histogram at the left one, but using *.stem()* function rather *.plot()* for this time.\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "# 1. Define parameter to extract LBP feature in (8, 1) neighborhood:\n",
    "#    1.1 Please set the number of neighbour P = 8\n",
    "#    1.2 Please set the radius if circir R = 1.0\n",
    "#    2.3 Please set the method as 'nri_uniform'\n",
    "\n",
    "\n",
    "# 2. Extracting the LBP face using local_binary_pattern()\n",
    "\n",
    "\n",
    "# 3. Calculate the histogram of the LBP face. Sum of vector can be calculated by calling numpy.sum()\n",
    "\n",
    "\n",
    "# 4. Visualize your result.\n",
    "# \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Feature Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use Support Vector Machine (SVM) for facial expression classification. Please read the [`sklearn.svm.SVC()`]() and learn to uset it. Mainly you will use its two methods: **fit()** to training the classifier and **predict()** to use the classifer for classification. There are following three subtasks you need to complete:\n",
    "* Task 3.1. Load data\n",
    "* Task 3.2. Train classifiers\n",
    "* Task 3.3. Evaluate classifiers\n",
    "\n",
    "\n",
    "\n",
    "### Task 3.1. Load data\n",
    "Firstly, you need to read *.mat* files using python. You can use the [`scipy.io.loadmat()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.loadmat.html#scipy.io.loadmat) function to read *.mat* file. In the provided **Task3_data.mat** file, different data are packed by different dictionaries which are list below:\n",
    "* training_data\n",
    "* testing_data\n",
    "* training_class\n",
    "* testing_class\n",
    "For example if you would like to load the *training_data* from the file, you can use **scipy.io.loadmat('Task3_data.mat')['training_data']**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scipy.io as sio\n",
    "\n",
    "# Loading data using scipy.io.loadmat(), or sio.loadmat\n",
    "# Alternatively, you can us h5py also, if you would like to\n",
    "mdata = sio.loadmat('Task3_data.mat')\n",
    "\n",
    "\n",
    "# Load 'training_data'\n",
    "sample_train = mdata['training_data']\n",
    "\n",
    "\n",
    "# Load 'testing_data'\n",
    "\n",
    "\n",
    "# Load 'training_class'\n",
    "\n",
    "\n",
    "# Load 'testing_class'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3. Train SVM classifiers\n",
    "Use the **sklearn.svm** library to train Support Vector Machine (SVM) classifiers. The ‘training_data’ and ‘testing_data’ matrices contain the calculated LBP-TOP features for the training and testing sets, respectively. The block size for LBP-TOP used for training and testing data are 2x2x1. The ‘training_class’ group vector contains the class of samples: 1 = happy, 2 = sadness, corresponding to the rows of the training data matrices.\n",
    "\n",
    "**Steps**:\n",
    "1.  Construct an SVM classifier using the linear kernel. Please read [`sklearn.svm.SVC()`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC).\n",
    "\n",
    "2.  Use the [`fit()`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.fit) method and the *training_data* and *training_class* to train your classifier.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "# 1. Initializing a SVM classifier, using linear kernel\n",
    "clf = svm.SVC(kernel  = 'linear', cache_size = 5000)\n",
    "\n",
    "# 2. using the classifier to fit your training data\n",
    "clf.fit(sample_train, label_train)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3. Evaluate your classifiers\n",
    "**Steps**:\n",
    "1. Use your trainined classifer to classify the *training_data* and *testing_data*, using the [`predict()`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.predict) method.\n",
    "2. Calculate the classification accuricies when classifying the *training_data* and *testing_data*, respectively. The correct class labels corresponding with the rows of the training and testing data matrices are in the variables *training_class* and *testing_class*, respectively.\n",
    "3. Calculate the confusion matrices when evaluating either dataset. Using [`sklearn.metrics.confusion_matrix()`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Predicting you training data and testing data.\n",
    "prediction_train = clf.predict(sample_train)\n",
    "prediction = clf.predict(sample_test)\n",
    "\n",
    "# 2. Calculating the accuracies of your prediction on training data and testing data, respectively.\n",
    "#    2.1 calculate the accuracy when classifying the training data \n",
    "\n",
    "\n",
    "#    2.2 calculate the accuracy when classifying the test data\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 3. Draw your confusion matrix\n",
    "# 3. using sklearn.metrics.confusion_matrix\n",
    "#    3.1 Calculate the confusion matrix when classifying the training data\n",
    "\n",
    "\n",
    "#    3.2 Calculate the confusion matrix when classifying the testing data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
